{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ho_na7Tl78m6",
        "outputId": "3b810aef-c906-4e9c-de62-2ca90df3dc5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "========== 1) Character-level Tokenization ==========\n",
            "Original Text: Teja is good at LLM\n",
            "Character Tokens: ['T', 'e', 'j', 'a', ' ', 'i', 's', ' ', 'g', 'o', 'o', 'd', ' ', 'a', 't', ' ', 'L', 'L', 'M']\n",
            "Total Characters: 19\n",
            "\n",
            "========== 2) Subword Tokenization (SentencePiece BPE) ==========\n",
            "['▁', 's', 'u', 'b', 'w', 'or', 'd', '▁t', 'ok', 'en', 'iz', 'at', 'io', 'n', '▁', 'h', 'e', 'l', 'p', 's']\n",
            "\n",
            "========== 3) Word2Vec + Vocabulary Size ==========\n",
            "Vocab size: 6\n",
            "\n",
            "Embedding for word: nlp\n",
            "[-0.00268114  0.00118216  0.02551675  0.04504637 -0.04651475 -0.03558404\n",
            "  0.03229436  0.04486494 -0.02507714 -0.01881686  0.03690252 -0.00766736\n",
            " -0.02268307  0.03277026 -0.0243008  -0.00908009  0.0143829   0.00495937\n",
            " -0.04142607 -0.04724409]\n",
            "Embedding Dimension: 20\n",
            "\n",
            "========== 4) Vocabulary Size (NLTK word_tokenize) ==========\n",
            "Tokens: ['i', 'love', 'nlp', 'and', 'i', 'love', 'tokenization']\n",
            "Vocabulary: {'love', 'i', 'tokenization', 'nlp', 'and'}\n",
            "Vocabulary Size: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "!pip install gensim\n",
        "\n",
        "text1 = \"Teja is good at LLM\"\n",
        "char_tokens = list(text1)\n",
        "\n",
        "print(\"========== 1) Character-level Tokenization ==========\")\n",
        "print(\"Original Text:\", text1)\n",
        "print(\"Character Tokens:\", char_tokens)\n",
        "print(\"Total Characters:\", len(char_tokens))\n",
        "\n",
        "\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "\n",
        "open(\"data.txt\", \"w\").write(\"tokenization is important\\nsubword tokenization helps\\n\")\n",
        "\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=\"data.txt\",\n",
        "    model_prefix=\"m\",\n",
        "    vocab_size=30,\n",
        "    model_type=\"bpe\"\n",
        ")\n",
        "\n",
        "sp = spm.SentencePieceProcessor(model_file=\"m.model\")\n",
        "\n",
        "print(\"\\n========== 2) Subword Tokenization (SentencePiece BPE) ==========\")\n",
        "print(sp.encode(\"subword tokenization helps\", out_type=str))\n",
        "\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "data = [\n",
        "    [\"i\", \"love\", \"nlp\"],\n",
        "    [\"nlp\", \"is\", \"easy\"],\n",
        "    [\"i\", \"love\", \"word2vec\"]\n",
        "]\n",
        "\n",
        "\n",
        "model = Word2Vec(data, vector_size=20, min_count=1)\n",
        "\n",
        "print(\"\\n========== 3) Word2Vec + Vocabulary Size ==========\")\n",
        "print(\"Vocab size:\", len(model.wv.index_to_key))\n",
        "\n",
        "\n",
        "sample_word = \"nlp\"\n",
        "print(\"\\nEmbedding for word:\", sample_word)\n",
        "print(model.wv[sample_word])\n",
        "print(\"Embedding Dimension:\", len(model.wv[sample_word]))\n",
        "\n",
        "\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "text2 = \"I love NLP and I love tokenization\"\n",
        "tokens = word_tokenize(text2.lower())\n",
        "vocab = set(tokens)\n",
        "\n",
        "print(\"\\n========== 4) Vocabulary Size (NLTK word_tokenize) ==========\")\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"Vocabulary Size:\", len(vocab))"
      ]
    }
  ]
}